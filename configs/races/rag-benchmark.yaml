# RAG Benchmark Race
# Test long-context processing performance

race:
  name: "RAG Benchmark - Long Context Test"
  
  # RAG-specific parameters
  duration: 900  # 15 minutes
  users: 20      # Fewer users due to long context
  spawn_rate: 2.0
  
  # Long context workload (BillSum dataset)
  dataset: "billsum"
  target_input_tokens: 4096  # Long context
  target_output_tokens: 512
  
  output_dir: "results/races"
  
  # Engines optimized for long context
  engines:
    - name: "vllm-rag"
      url: "http://localhost:8000"
      model: "meta-llama/Llama-3.2-3B-Instruct"
      emoji: "ðŸ“š"
      color: "blue"
      tokenizer: "meta-llama/Llama-3.2-3B-Instruct"
    
    - name: "tgi-rag"
      url: "http://localhost:8001"
      model: "meta-llama/Llama-3.2-3B-Instruct"
      emoji: "ðŸ“–"
      color: "cyan"
      tokenizer: "meta-llama/Llama-3.2-3B-Instruct"

