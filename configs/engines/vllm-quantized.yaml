# vLLM Quantized - Cost-Optimized Configuration
# Purpose: Benchmark cost-optimized quantized model
# Last Updated: 2025-10-06
# Benchmark ID: Performance vs Quantized comparison

kind: Deployment
apiVersion: apps/v1
metadata:
  name: vllm-quantized
  namespace: vllm-benchmark
  labels:
    app.kubernetes.io/name: vllm
    app.kubernetes.io/instance: vllm-quantized
    vllm.config/name: quantized
  annotations:
    cost.optimization: quantized
    quantization.type: awq
    kv.cache.dtype: fp8
    gpu.memory.target: 60-percent
    features: 'prefix-caching,v2-block-manager,priority-scheduling'

spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
      app.kubernetes.io/instance: vllm-quantized

  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm
        app.kubernetes.io/instance: vllm-quantized
      annotations:
        cost.optimization: quantized

    spec:
      containers:
        - name: vllm
          image: 'docker.io/vllm/vllm-openai:latest'

          resources:
            requests:
              cpu: '2'
              memory: 16Gi
              nvidia.com/gpu: '1'
            limits:
              cpu: '4'
              memory: 24Gi
              nvidia.com/gpu: '1'

          args:
            - '--model=RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8'
            - '--port=8000'
            - '--host=0.0.0.0'
            - '--dtype=auto'
            - '--kv-cache-dtype=fp8'  # FP8 for cost savings
            - '--calculate-kv-scales'
            - '--gpu-memory-utilization=0.60'  # 60% - cost optimized
            - '--swap-space=8'
            - '--max-model-len=4096'
            - '--max-num-batched-tokens=6144'
            - '--max-num-seqs=96'  # More sequences (cost efficient)
            - '--enable-prefix-caching'
            - '--prefix-caching-hash-algo=sha256'
            - '--scheduling-policy=priority'
            - '--num-lookahead-slots=4'
            - '--disable-log-stats'
            - '--block-size=16'

          ports:
            - name: http
              containerPort: 8000
              protocol: TCP

          env:
            - name: HF_HOME
              value: /models/.cache/huggingface
            - name: VLLM_WORKER_MULTIPROC_METHOD
              value: spawn
            - name: CUDA_VISIBLE_DEVICES
              value: '0'
            - name: NCCL_DEBUG
              value: WARN

          volumeMounts:
            - name: model-cache
              mountPath: /models
            - name: dshm
              mountPath: /dev/shm

      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-quantized-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 2Gi  # Smaller for cost savings

      nodeSelector:
        nvidia.com/gpu.present: 'true'

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

