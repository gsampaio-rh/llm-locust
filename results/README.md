# Results Directory

This directory stores test output files generated by LLM Locust.

## File Types

### Per-Request Metrics (CSV)
Default: `per_request_metrics.csv`

**Format:**
```csv
request_id,timestamp,user_id,user_request_num,input_tokens,output_tokens,ttft_ms,tpot_ms,end_to_end_s,total_tokens_per_sec,output_tokens_per_sec,status_code
1,1759519472,0,1,256,128,234.50,12.30,1.847,207.98,69.28,200
```

**Fields:**
- `request_id` - Global request ID across all users
- `timestamp` - Unix timestamp when request completed
- `user_id` - Which simulated user made this request (0-N)
- `user_request_num` - Request number for this specific user
- `input_tokens` - Number of input tokens (prompt length)
- `output_tokens` - Number of generated tokens
- `ttft_ms` - Time to First Token in milliseconds
- `tpot_ms` - Time Per Output Token in milliseconds
- `end_to_end_s` - Total request duration in seconds
- `total_tokens_per_sec` - Throughput (input + output tokens per second)
- `output_tokens_per_sec` - Generation throughput (output tokens per second)
- `status_code` - HTTP status code (200 = success)

### Per-Request Metrics (JSONL)
Alternative format: `per_request_metrics.jsonl`

Each line is a complete JSON object with all fields:
```json
{"request_id": 1, "timestamp": 1759519472, "user_id": 0, ...}
```

## Analysis Examples

### Load and Analyze with Pandas

```python
import pandas as pd

# Load results
df = pd.read_csv('results/per_request_metrics.csv')

# Basic statistics
print(df.describe())

# Per-user analysis
print(df.groupby('user_id').agg({
    'ttft_ms': ['mean', 'std', 'min', 'max'],
    'tpot_ms': ['mean', 'std'],
    'end_to_end_s': ['mean', 'max']
}))

# Find slow requests
slow_requests = df.nlargest(10, 'ttft_ms')
print("Slowest requests by TTFT:")
print(slow_requests[['request_id', 'user_id', 'ttft_ms', 'input_tokens']])

# Check SLA compliance
sla_ttft_ms = 500
violations = df[df['ttft_ms'] > sla_ttft_ms]
print(f"SLA violations: {len(violations)}/{len(df)} ({len(violations)/len(df)*100:.1f}%)")
```

### Performance Over Time

```python
import matplotlib.pyplot as plt

# Plot TTFT over time
df.plot(x='request_id', y='ttft_ms', kind='scatter', alpha=0.5)
plt.title('TTFT Over Time')
plt.xlabel('Request ID')
plt.ylabel('TTFT (ms)')
plt.show()

# Plot by user
for user_id in df['user_id'].unique():
    user_data = df[df['user_id'] == user_id]
    plt.plot(user_data['request_id'], user_data['ttft_ms'], 
             label=f'User {user_id}', alpha=0.7)
plt.legend()
plt.title('TTFT by User')
plt.show()
```

### Compare Test Runs

```python
# Load multiple test results
run1 = pd.read_csv('results/test1_results.csv')
run2 = pd.read_csv('results/test2_results.csv')

# Compare means
print("TTFT Comparison:")
print(f"  Run 1: {run1['ttft_ms'].mean():.1f}ms")
print(f"  Run 2: {run2['ttft_ms'].mean():.1f}ms")
print(f"  Improvement: {((run1['ttft_ms'].mean() - run2['ttft_ms'].mean()) / run1['ttft_ms'].mean() * 100):.1f}%")
```

## File Management

### Naming Convention

Recommended naming for multiple tests:
```
results/
├── baseline_dolly_10users.csv
├── chat_sharegpt_20users.csv
├── prefill_billsum_5users.csv
├── decode_infinity_10users.csv
└── cache_test_sharedprefix.csv
```

### Cleanup

Old results are **not** automatically deleted. Clean up manually:
```bash
# Remove all results
rm results/*.csv results/*.jsonl

# Or keep recent ones
ls -lt results/*.csv | tail -n +6 | awk '{print $9}' | xargs rm
```

## Tips

1. **Always use unique filenames** for different tests to avoid overwriting
2. **CSV format** is easier for pandas/Excel analysis
3. **JSONL format** preserves full data types
4. **Check headers** - if missing, delete file and re-run
5. **Timestamp sorting** - results are ordered by completion time, not start time

---

See [METRICS_COVERAGE.md](../docs/METRICS_COVERAGE.md) for detailed metrics analysis examples.

