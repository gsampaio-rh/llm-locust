# LLM Benchmark Comparison Dashboard

A Streamlit-based dashboard for comparing LLM inference benchmark results across multiple serving platforms.

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd streamlit_app
pip install -r requirements.txt
```

### 2. Run the Dashboard

```bash
streamlit run app.py
```

The dashboard will open in your browser at `http://localhost:8501`

### 3. Upload Benchmark Data

1. Run benchmarks using llm-locust:
   ```bash
   python examples/benchmark_chat_simulation.py \
       --host https://your-endpoint.com \
       --model your-model \
       --engine vllm
   ```

2. Upload the generated CSV files from `results/` directory

3. Explore the analysis pages!

## ğŸ“ Project Structure

```
streamlit_app/
â”œâ”€â”€ app.py                  # Main entry point
â”œâ”€â”€ config.py              # Configuration and constants
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md             # This file
â”‚
â”œâ”€â”€ pages/                # Multi-page sections
â”‚   â”œâ”€â”€ 1_ğŸ“Š_Overview.py
â”‚   â”œâ”€â”€ 2_âš¡_Latency_Analysis.py
â”‚   â”œâ”€â”€ 3_ğŸš€_Throughput_Analysis.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ lib/                  # Business logic
â”‚   â”œâ”€â”€ data_loader.py    # CSV loading and validation
â”‚   â”œâ”€â”€ metrics.py        # Metric calculations
â”‚   â”œâ”€â”€ statistics.py     # Statistical tests
â”‚   â””â”€â”€ visualizations.py # Chart generation
â”‚
â”œâ”€â”€ models/               # Data models
â”‚   â””â”€â”€ benchmark.py      # BenchmarkData, Metadata, etc.
â”‚
â””â”€â”€ utils/               # Utilities
    â”œâ”€â”€ validators.py    # Data validation
    â””â”€â”€ formatters.py    # Display formatting
```

## ğŸ¯ Features

### Current (MVP)

- âœ… **File Upload**: Drag-and-drop CSV files
- âœ… **Data Validation**: Schema checking and quality scoring
- âœ… **Overview Dashboard**: Quick comparison table and winner metrics
- âœ… **Metadata Extraction**: Auto-detect platform from filenames

### Coming Soon

- ğŸš§ **Latency Analysis**: Distributions, percentiles, statistical tests
- ğŸš§ **Throughput Analysis**: Time series, stability metrics
- ğŸš§ **Error Analysis**: Failure patterns and reliability scoring
- ğŸš§ **Cost Analysis**: TCO comparison (optional)
- ğŸš§ **Report Generation**: Export PDF summaries

## ğŸ“Š Expected CSV Format

The dashboard expects CSV files with the following columns:

**Required:**
- `request_id`, `timestamp`, `user_id`, `user_request_num`
- `input_tokens`, `output_tokens`
- `ttft_ms`, `tpot_ms`, `end_to_end_s`
- `total_tokens_per_sec`, `output_tokens_per_sec`
- `status_code`

**Optional:**
- `input_prompt`, `output_text`

**Filename Format:** `{platform}-{YYYYMMDD}-{HHMMSS}-{benchmark-id}.csv`

Example: `vllm-20251003-175002-1a-chat-simulation.csv`

## ğŸ”§ Configuration

Edit `config.py` to customize:

- File upload limits
- Chart styling and colors
- Statistical thresholds
- SLA criteria
- Default percentiles

## ğŸ§ª Development

### Run Tests

```bash
pytest tests/
```

### Type Checking

```bash
mypy streamlit_app/
```

### Linting

```bash
ruff check streamlit_app/
```

## ğŸ“ Usage Example

1. **Run benchmarks** on multiple platforms:
   ```bash
   # vLLM
   python examples/benchmark_chat_simulation.py --engine vllm --host ...

   # TGI
   python examples/benchmark_chat_simulation.py --engine tgi --host ...
   ```

2. **Upload CSVs** to dashboard (both files)

3. **Compare results** across all analysis pages

4. **Make decisions** based on data (latency vs throughput trade-offs)

## ğŸ¤ Contributing

Contributions welcome! Follow the project's coding standards and add tests for new features.

## ğŸ“„ License

Same as parent project (llm-locust)

## ğŸ™ Credits

Built with:
- [Streamlit](https://streamlit.io)
- [Plotly](https://plotly.com/python/)
- [Pandas](https://pandas.pydata.org)
- [SciPy](https://scipy.org)

